# =====================================================================
# 21_test_plan.ir.yml
# =====================================================================
# ⚠️ WARNING: THIS IS A TEMPLATE FOR A SMOKE TEST PLAN IR.
#
# PURPOSE:
#   - Define the SMOKE TEST PLAN for the system:
#       • Minimal, fast, high-value tests
#       • Answer the question: "Is the system basically alive and sane?"
#   - Link architecture (10_architecture.ir.yml), implementation plan
#     (20_impl_plan.ir.yml), and code status (30_code_status.ir.yml)
#     to concrete smoke test cases.
#
# VERSIONING & HISTORY:
#   - This file MUST always represent the LATEST smoke test plan.
#   - When making significant changes:
#       1) COPY the current file to:
#            spec/history/21_test_plan_YYYY-MM-DD.yml
#            or
#            spec/history/21_test_plan_vNN.yml
#       2) THEN edit this file (21_test_plan.ir.yml) for the new plan.
#   - Files in spec/history/ are READ-ONLY snapshots:
#       • NEVER edit old plans.
#       • Use them for audits, regression of the plan itself, and archaeology.
#
# LLM USAGE:
#   - Gemini/Codex/Claude MUST:
#       • Read this file to know which smoke tests are required.
#       • Prefer implementing/maintaining these tests FIRST.
#       • Update this file ONLY after smoke tests are actually added/changed.
#
# TEMPLATE NOTICE:
#   - All IDs, names, and examples below are placeholders.
#   - YOU MUST adapt them to your actual system (modules, workflows, commands).
# =====================================================================

metadata:
  system_name: TODO_replace_with_system_name          # e.g. "robo_safety_monitor"
  plan_version: 0.1.0
  last_updated: "TODO_YYYY-MM-DD"
  last_updated_by: TODO_author
  based_on_architecture_version: "0.1.0"
  based_on_impl_plan_version: "0.1.0"
  based_on_code_status_version: "0.1.0"
  notes:
    - "This is the canonical smoke test plan; other test plans may exist separately."
    - "Smoke tests should be fast (ideally < 5–10 minutes total)."

# -------------------------------------------------------------------
# 1. SMOKE TEST PHILOSOPHY & SCOPE
# -------------------------------------------------------------------

philosophy:
  definition: >
    Smoke tests are a minimal set of automated checks that verify whether
    the system is in a basically usable state. They do NOT aim for high
    coverage, but instead for high SIGNAL: if a smoke test fails, the
    system is not safe to proceed with deeper testing or demo usage.
  goals:
    - "Detect glaring breakages in critical modules early."
    - "Run quickly enough to be executed on every commit or before each deployment."
    - "Be stable and deterministic (no flaky behavior)."
  non_goals:
    - "Comprehensive coverage of all edge cases."
    - "Full performance benchmarking."
    - "Long-running robustness or stress testing."

scope:
  included:
    - "Core data flow across main modules (HAL → Perception → Core Logic → Connectivity → Backend)."
    - "At least one critical workflow end-to-end."
    - "Basic health checks for each critical module."
  excluded:
    - "Exhaustive ML model evaluation."
    - "Full-scale load tests or long-duration endurance tests."

# -------------------------------------------------------------------
# 2. ENVIRONMENTS & EXECUTION CONTEXT
# -------------------------------------------------------------------

environments:
  # ⚠️ TEMPLATE: define minimal environments where smoke tests must pass.
  - id: dev_local
    description: "Developer laptop environment (Linux or macOS)."
    requirements:
      - "Python 3.10+ installed."
      - "C++ build toolchain available (gcc/clang + CMake)."
      - "No hardware required; mocks or simulators are used."
    trigger:
      - "Run on every main branch commit (CI)."
      - "Run before local feature branches are merged."

  - id: robot_edge
    description: "On-robot or edge computing device."
    requirements:
      - "Basic sensor mocks or safe test configuration available."
      - "Network connectivity to local broker/backend if required."
    trigger:
      - "Run before deployment to production robots."
      - "Run after major system upgrades."

  # Add more environments as needed.

execution_policies:
  max_total_duration_minutes: 10
  parallelization_allowed: true
  must_be_automated: true
  manual_steps_allowed: false   # smoke tests should be fully automated

# -------------------------------------------------------------------
# 3. TEST LEVELS (WITH SMOKE-FOCUSED LAYER)
# -------------------------------------------------------------------

test_levels:
  - name: smoke
    description: "Minimal, critical-path tests executed frequently."
    required_for_merge: true
    required_for_release: true
  - name: unit
    description: "Per-function or per-class tests (not fully described here)."
    required_for_merge: false
    required_for_release: true
  - name: integration
    description: "Module interaction tests (beyond minimal smoke)."
    required_for_merge: false
    required_for_release: case_by_case
  - name: e2e
    description: "Full end-to-end scenarios; may be heavier than smoke."
    required_for_merge: false
    required_for_release: case_by_case

# -------------------------------------------------------------------
# 4. SMOKE TEST CASE TEMPLATE (REFERENCE)
# -------------------------------------------------------------------
# This section describes the STRUCTURE of a test case entry so
# humans and LLMs follow the same schema.
# -------------------------------------------------------------------

test_case_schema:
  fields:
    - { name: id,              type: string,  description: "Unique test case identifier." }
    - { name: name,            type: string,  description: "Short human-readable name." }
    - { name: level,           type: string,  description: "Test level, e.g. 'smoke'." }
    - { name: module_ids,      type: list,    description: "Affected modules (from 10_architecture.ir.yml)." }
    - { name: environment_ids, type: list,    description: "Applicable environments (from environments[])." }
    - { name: description,     type: string,  description: "What the test is validating at a high level." }
    - { name: preconditions,   type: list,    description: "What must be true before running the test." }
    - { name: steps,           type: list,    description: "Sequence of actions (commands, API calls, etc.)." }
    - { name: expected,        type: list,    description: "Expected observable outcomes." }
    - { name: automation,      type: object,  description: "How this test is automated (command, script, CI job)." }
    - { name: owner,           type: string,  description: "Responsible person/role." }
    - { name: status,          type: string,  description: "Test implementation status: planned/in_progress/ready." }
    - { name: last_run,        type: string,  description: "Timestamp of last successful execution (if known)." }

# -------------------------------------------------------------------
# 5. MODULE-CENTRIC SMOKE TEST MATRIX
# -------------------------------------------------------------------
# Minimal smoke tests per critical module.
# These are *not* all tests — just the ones that MUST always pass.
# -------------------------------------------------------------------

module_smoke_matrix:
  # ========================
  # Example: hal_encoder
  # ========================
  - module_id: hal_encoder
    importance: "high"
    rationale: >
      Without basic encoder functionality, motion-derived state is unreliable.
      A quick smoke test is required to ensure data flows and basic computations work.
    smoke_tests:
      - id: SMK_HAL_001
        name: "EncoderAdapter basic init & tick-to-velocity conversion"
        level: "smoke"
        environment_ids: ["dev_local"]
        description: >
          Build and run a minimal test that instantiates EncoderAdapter,
          feeds it a small sequence of fake ticks, and verifies that the
          computed velocity is finite, non-NaN, and within expected bounds.
        preconditions:
          - "C++ toolchain and dependencies installed."
          - "No physical hardware required; test uses mock inputs."
        steps:
          - "Build the hal_encoder target via CMake."
          - "Run the test binary 'test_encoder_adapter_basic'."
        expected:
          - "Build succeeds without errors."
          - "Test process exits with code 0."
          - "No assertion failures; logs show valid velocities."
        automation:
          command: "ctest -R test_encoder_adapter_basic"
          ci_job: "hal_encoder_smoke"
        owner: "TODO_team_or_person"
        status: "planned"          # planned | in_progress | ready
        last_run: null

  # ========================
  # Example: perception_hazard_detector
  # ========================
  - module_id: perception_hazard_detector
    importance: "high"
    rationale: >
      Hazard detection is central to the safety purpose of the system.
      Even a minimal smoke test must ensure the detector pipeline runs
      end-to-end with mock data without crashing.
    smoke_tests:
      - id: SMK_PER_001
        name: "HazardDetector can process one frame without crashing"
        level: "smoke"
        environment_ids: ["dev_local"]
        description: >
          Run the hazard detector on a single synthetic or recorded frame and
          verify that it returns a result (possibly empty) within a short timeout.
        preconditions:
          - "Python environment with required packages installed."
          - "Test image or synthetic input available."
        steps:
          - "Run 'python -m perception_hazard_detector.smoke_test_one_frame'."
        expected:
          - "Process exits with code 0."
          - "Runtime is below a configured threshold (e.g., 2 seconds)."
          - "Output file or log confirms detection function returned successfully."
        automation:
          command: "pytest tests/smoke/test_hazard_one_frame.py -q"
          ci_job: "perception_smoke"
        owner: "TODO_team_or_person"
        status: "planned"
        last_run: null

  # ========================
  # Example: core_patrol_manager
  # ========================
  - module_id: core_patrol_manager
    importance: "high"
    rationale: >
      PatrolManager coordinates robot behavior based on state and events.
      A smoke test must confirm the basic state machine functions and
      transitions between key states without error.
    smoke_tests:
      - id: SMK_CORE_001
        name: "PatrolManager basic start/stop cycle"
        level: "smoke"
        environment_ids: ["dev_local"]
        description: >
          Instantiate PatrolManager with mocked robot state and issue a
          start/stop command sequence, ensuring no crashes and correct
          final state.
        preconditions:
          - "Core library compiled successfully."
        steps:
          - "Run 'ctest -R test_patrol_manager_smoke'."
        expected:
          - "Test process exits with code 0."
          - "State transitions logged as expected (IDLE → PATROL → IDLE)."
        automation:
          command: "ctest -R test_patrol_manager_smoke"
          ci_job: "core_smoke"
        owner: "TODO_team_or_person"
        status: "planned"
        last_run: null

  # ========================
  # Example: connectivity_bridge
  # ========================
  - module_id: connectivity_bridge
    importance: "high"
    rationale: >
      Connectivity bridge is the artery between internal system and external
      world. A smoke test should confirm it can start, connect to a test
      broker (or mock), and forward at least one message.
    smoke_tests:
      - id: SMK_CONN_001
        name: "ConnectivityBridge start & one message round-trip"
        level: "smoke"
        environment_ids: ["dev_local"]
        description: >
          Start the bridge with a mock transport, publish a test RobotState
          message internally, and verify it appears on the external side.
        preconditions:
          - "Python environment and mock broker installed."
        steps:
          - "Start mock broker or simulated transport."
          - "Run 'python -m connectivity.bridge.smoke_test_roundtrip'."
        expected:
          - "Bridge initializes and connects without exceptions."
          - "Test message is observed on the external side."
        automation:
          command: "pytest tests/smoke/test_connectivity_roundtrip.py -q"
          ci_job: "connectivity_smoke"
        owner: "TODO_team_or_person"
        status: "planned"
        last_run: null

  # ========================
  # Example: app_backend_api
  # ========================
  - module_id: app_backend_api
    importance: "medium"
    rationale: >
      Backend API is critical for operators but can initially rely on
      simple smoke checks: server starts and responds to a basic /health
      endpoint.
    smoke_tests:
      - id: SMK_APP_001
        name: "Backend API server health endpoint"
        level: "smoke"
        environment_ids: ["dev_local"]
        description: >
          Start the backend API server in a test mode and query the health
          endpoint, verifying a 200 OK response and expected payload.
        preconditions:
          - "Node/TypeScript environment installed."
        steps:
          - "Run 'npm run start:test' in src/app/backend_api."
          - "From another process, call 'curl http://localhost:8080/health'."
        expected:
          - "Server starts without crashing."
          - "Health endpoint returns HTTP 200 with status 'OK'."
        automation:
          command: "npm run smoke"
          ci_job: "backend_smoke"
        owner: "TODO_team_or_person"
        status: "planned"
        last_run: null

# -------------------------------------------------------------------
# 6. CRITICAL WORKFLOW SMOKE SCENARIOS (END-TO-END)
# -------------------------------------------------------------------
# Minimal end-to-end tests across multiple modules.
# -------------------------------------------------------------------

workflow_smoke:
  - id: WF_SMK_001
    name: "Hazard detection end-to-end, simulated data"
    description: >
      Simulate a robot moving through an environment, feed synthetic sensor
      data, run the perception and core logic, and verify that a SafetyEvent
      reaches the connectivity layer or backend endpoint.
    involved_modules:
      - hal_encoder
      - perception_hazard_detector
      - core_patrol_manager
      - connectivity_bridge
    environment_ids: ["dev_local"]
    preconditions:
      - "All involved services runnable in mock mode."
      - "Test configuration loaded (no real hardware required)."
    steps:
      - "Start local mock runtime (e.g., docker-compose up test_stack)."
      - "Inject synthetic robot_state and sensor data into perception inputs."
      - "Run system for a short time window (e.g., 5–10 seconds)."
      - "Capture messages from safety_event_stream or backend /events endpoint."
    expected:
      - "At least one SafetyEvent is produced with severity != null."
      - "No process crashes or unrecoverable errors logged."
    automation:
      command: "pytest tests/smoke/test_e2e_hazard_detection.py -q"
      ci_job: "e2e_smoke"
    owner: "TODO_team_or_person"
    status: "planned"
    last_run: null

# -------------------------------------------------------------------
# 7. CI INTEGRATION & EXECUTION ORDER
# -------------------------------------------------------------------
# Define how smoke tests are stitched into CI/CD or local workflows.
# -------------------------------------------------------------------

ci_integration:
  pipelines:
    - name: "main_branch_smoke"
      trigger: "push_to_main"
      environment_id: "dev_local"
      steps:
        - "Build core components (C++ libraries, Python packages, backend)."
        - "Run all module_smoke_matrix tests with level == 'smoke'."
        - "Run all workflow_smoke tests."
      fail_policy: "fail_fast"   # fail_fast | run_all
      artifact_retention: "7d"

    - name: "pre_release_smoke"
      trigger: "manual_or_tagged_release"
      environment_id: "robot_edge"
      steps:
        - "Deploy release candidate to edge device or staging robot."
        - "Run selected subset of smoke tests adapted to edge hardware."
      fail_policy: "run_all"

local_execution:
  recommended_commands:
    - description: "Run all smoke tests locally (developer machine)."
      command: "pytest tests/smoke -q && ctest -R '.*smoke.*'"
    - description: "Run only perception smoke tests."
      command: "pytest tests/smoke/test_hazard_* -q"

# -------------------------------------------------------------------
# 8. MAINTENANCE & REVIEW POLICY
# -------------------------------------------------------------------

maintenance:
  review_frequency: "monthly"     # how often the smoke plan itself should be reviewed
  responsible_roles:
    - "test_owner"
    - "system_architect"
  rules:
    - "Any new critical module in 10_architecture.ir.yml MUST have at least one smoke test defined here."
    - "If a smoke test becomes flaky, it must be fixed or redesigned; flaky smoke tests are forbidden."
    - "Before deprecating a module, its smoke tests should be clearly marked as deprecated or removed."